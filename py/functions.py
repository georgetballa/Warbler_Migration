import os.path
from os import path
import pickle
import pandas as pd
from ebird.api import *
import matplotlib.pyplot as plt
from datetime import *
import datetime as dt
import re
import requests
import numpy as np
from dateutil.relativedelta import relativedelta
import gc
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import statsmodels.api as sm
from glm.glm import GLM
from glm.families import Gaussian
import scipy
import scipy.stats as stats
from matplotlib.dates import (YEARLY, DateFormatter,rrulewrapper, RRuleLocator, drange)


def hist2pickle(country, year, month, day, api_key):
    '''
    
    Converts Received Data to Pickle File
    
    Parameters:
    --------------
    Country Code
    Year
    Month
    Day
    species: species code
    api_key
    
    Returns:
    -----------
    Pickle File Named by Year-Month-Day  
        
    '''
    url = f"https://api.ebird.org/v2/data/obs/{country}/historic/{year}/{month}/{day}"

    payload = {
        'detail':'full',
        
    }
    headers = {'X-eBirdApiToken': api_key}
    
    response = requests.request("GET", url, headers=headers, data = payload)
    
    df = pd.read_json(response.text.encode('utf8'))

    pickle_out = open(f'./pickles/{year}-{month}-{day}','wb')
    pickle.dump(df,pickle_out)
    pickle_out.close()
    
def period_of_observations(api_key, country, start_date, end_date):
    '''
    Requests observations from EBIRD API for each date in the range
    
    Parameters
    ----------
    country: Which country are the observations from
    start_date: beginning of period
    end_date: end of period
    api_key: ebird API
    
    Returns
    ----------
    Pickle File for each day in range saved in ./pickles directory
    '''
    for n in range(int(((end_date) - start_date).days)):
        name = start_date + timedelta(n)
        strname = str(name)
        full_df = hist2pd(api_key, country, year = name.strftime('%Y'), month = name.strftime("%m"), day = name.strftime("%d"))

        
def obsformatter(col, search, del_list, date):
    '''
    Search an observation day pickle file, filter by column for string, return pickle df for that day
    If column is not in pickle df, file is skipped
    
    
    Parametersjan2020
    ----------
    country: Which country are the observations from
    start_date: beginning of period
    end_date: end of period
    api_key: ebird API
    
    Returns
    ----------
    Pickle File for each day in range saved in ./pickles directory
    '''
    pickle_in = open(f'./pickles/{date}','rb')
    df = pickle.load(pickle_in)
    missed = []
    if col in list(df.columns):
        df_loaded = df[df[f'{col}'] == search].copy()
    else:
        df_loaded = df 
        
    df_loaded.reset_index(inplace=True)
    
    for i in del_list:
    
        if i in list(df_loaded.columns):
            df_loaded.drop([i], axis=1, inplace=True)
        else:
            None
        
       
    if col in list(df_loaded.columns):
        pickle_out = open(f'./pickles/{search}/{date}_{search}','wb')
        pickle.dump(df_loaded,pickle_out)
        pickle_out.close() 
    else:
        missed = f'{date}'
        return missed
    
def batch_search_to_df(col, search, start_date, end_date, del_list =['index','obsReviewed','subId','locationPrivate']):
    '''
    Uses obsformatter function to format range of dates of Pickle Files   
    
    Parameters
    ----------
    start_date: first day to start formatting
    end_date: last day to format(non inclusive)
    col: column being searched
    search: string being searched in each col
    del_list: list of columns to delete
        
    Returns
    ----------
    Formatted Pickle File for each day in range
    File named with search term trailing
    Put in folder named using search term
    '''
    lst = []
    
    for n in range(int(((end_date) - start_date).days)):
        name = start_date + timedelta(n)
        strname = str(name)
        missed = obsformatter(col, search, del_list, date = f'{strname}')
        lst.append(missed)
        
    ### Make a Missing Dates File    
    lst = list(filter(None, lst))    
    year = start_date.strftime("%Y")    
    pickle_out = open(f'./pickles/{search}/{year}_missing','wb')
    pickle.dump(lst,pickle_out)
    pickle_out.close() 
        
        
def request_from_missing(api_key, missing_list, country):
    '''
    Parameters
    -----------
    missing_list: list of missing dates as generated by batch_search_to_df function
    country
    api_key
    
    Return
    -----------
    Pickles of missing dates
    
    '''
    for idx, i in enumerate(missing_list):
        year = missing_list[idx][0:4]
        month = missing_list[idx][5:7]
        day = missing_list[idx][8:10]
        hist2pickle(api_key, country, year, month, day)
        
    return

def year_to_csv(year):
    '''
    Take a Year of Pickle Data and Migrate to CSV
    Parameters
    ----------
    year: in date(2020,1,1) format
    
    Returns
    ---------
    Saves a CSV file   
    
    '''
    df = pd.DataFrame()
    
    year_str = year.strftime("%Y")
    
    for i in range(365):
        strname = str(year)
        if path.exists(f'./pickles/{strname}'):
            pickle_in = open(f'./pickles/{strname}','rb')
            df_date = pickle.load(pickle_in)
            df = df.append(df_date)        
            year = year + timedelta(1)      
                  
        else:
            year = year + timedelta(1)
   
    df.to_csv(f'./data/observations/{year_str}_observations')

def cleaner(year, search, col):
    ''' 
    Cleans Yearly Data to Begin Processing
    
    -Reads in CSV for that year
    -Drops NA's in your search column and the howMany column
    -Filters for your search string in the column
    -Converts obsDt to datetime
    -Sorts by date of observation
    -drops unused columns
    -creates the dtnum column which is an oordinal representation of dates for regression
    
    '''
    stryear = str(year)
    obs = pd.read_csv(f'./data/observations/{year}_observations')
    obs = obs.dropna(subset=[col, 'howMany'])
    filt = obs[col].str.lower().str.contains(f'{search}')
    obs = obs[filt]
    obs['obsDt'] = pd.to_datetime(obs['obsDt'])
    
    obs['dtnum'] = obs['obsDt'].map(dt.datetime.toordinal)
    obs = obs.sort_values(by='obsDt')
    
    obs = obs.reset_index()
    obs = obs.drop(['index','Unnamed: 0'], axis=1)
    return obs

def str_searcher(obs,search,col):
    '''
    Quick Search for Strings in Columns
        
    '''
    obs = obs[obs[col].str.lower().str.contains(f'{search}')]
    return obs
        
    
def date_searcher(df, start_date, end_date):
    '''
    Search dataframe between two dates
    
    '''
    start = pd.to_datetime(start_date)
    end = pd.to_datetime(end_date)
    mask = ((df['obsDt'] > start) & (df['obsDt'] <= end))
    df = df.loc[mask]
    
    return df
    
def total_cleaner(year):
    ''' 
    Cleans Yearly Data to Begin Processing
    No search filters, returns all observations for that year
    
    Parameter
    ---------
    year: date(2020,1,1)
    
    '''
    stryear = str(year)
    obs = pd.read_csv(f'./data/observations/{year}_observations')
    obs['obsDt'] = pd.to_datetime(obs['obsDt'])
    obs = obs.sort_values(by='obsDt')
#     obs = obs.dropna(subset=[col, 'howMany'])
#     filt = obs[col].str.lower().str.contains(f'{search}')
#     obs = obs[filt]
    obs = obs.reset_index()
    obs = obs.drop(['index','Unnamed: 0'], axis=1)
    return obs

def count_observations(obsdf, date):
    '''
    Count observations per month in any given year
    obsdf: dataframe of observations
    
     Parameter
    ---------
    date: date(2020,1,1)
    
    '''
    totals = {}
    for i in range(0,12):
        add_month = date + relativedelta(months=1)
        str_add_month = str(add_month)
        str_date = str(date)
        str_year = str_date[:4]
        df = date_searcher(obsdf, str_date, str_add_month)
        date = add_month
        totals[str_date] = len(df)
    return totals

def plot_observations_bar(df,df_total, year):
    
    '''
    Plot an observations bar graph comparing observation numbers
    
    Parameters
    --------------
    df: bottom bar data (output from count_observation function)
    df_total: top bar data (output from count_observation function)
    
    '''
    
    fig, ax = plt.subplots() 
    fig.set_size_inches(8, 6.5)
    x = np.array(list(df.keys()))
    y = np.array(list(df.values()))
    
    yt = np.array(list(df_total.values()))
    
    ax.bar(x,y, label = 'warblers', color='yellow')
    ax.bar(x,yt, bottom = y, label = 'total', color = 'saddlebrown')
    plt.xlabel('Month', fontsize=20)
    plt.ylabel('Observations', fontsize=20)
    plt.title(f'Warbler Observations {year}', fontsize = 20)
    plt.xticks(rotation=90, ha="right")
    plt.yticks(rotation=0)
    plt.legend()
    plt.tight_layout()
    fig.savefig(f'./graphs/monthly_distribution//{year}_warbler_observations.jpg', dpi=200);
    
def mean_lats(df):
    
    '''
    Calculate mean latitudes for each day in a dataframe
    
    Returns a dataframe grouped by dates with the means of the other columns
    '''
    
    mean_df = df[['speciesCode', 'comName','obsDt', 'lat', 'lng', 'dtnum']]
    mean_df['obsDt'] = mean_df['obsDt'].dt.strftime('%m-%d')
    mean_df = mean_df.groupby('obsDt').mean().copy()
    mean_df = mean_df.reset_index().copy()
    return mean_df

    def sample_from_repeated_sum(n_samples, n_summands, sampler):
    """Sample n_samples from the sum of n_summands
    Parameters
    ----------
    n_samples: number of sample lists to make
    n_summands: number of samples in each list
    Sampler: array of data to sample
    
    Returns:
    ----------
    sum of sampled points from each sampling
    """
    samples = np.random.choice(sampler, size=(n_samples, n_summands))
    return np.sum(samples, axis=1)

def sample_means_from_population(n_samples, n_summands, sampler):
    
    '''
    Calculate Means of sampled points
    Parameters
    ----------
    n_samples: number of sample lists to make
    n_summands: number of samples in each list
    Sampler: array of data to sample
    
    Returns:
    ----------
    A normal distribution of means
    '''
    
    return (1.0/n_summands) * sample_from_repeated_sum(n_samples, n_summands, sampler)